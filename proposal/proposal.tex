\documentclass[a4paper,12pt]{article}

\usepackage{authblk}
\usepackage[sort]{natbib}
\usepackage[utf8]{inputenc}
\usepackage{fullpage}
\usepackage{color}

\usepackage{indentfirst}

\newcommand{\etal}{et~al.}
\newcommand{\protosection}[1]{\vspace{1ex}\noindent\textbf{#1}}

%\linespread{0.96}
\title{\textbf{Automatic Parallelization for Heterogeneous Computing}}
\date{Prospective PhD supervisor - Dr. Zheng Wang}
\author[]{Rodrigo Caetano Rocha}

\newcommand\FIXME[1]{\textcolor{red}{FIX:}\textcolor{red}{#1}}

\begin{document}
\maketitle

\begin{abstract}

\end{abstract}

\section{Introduction} 

%\FIXME{ZW: The introduction seems to be too long. I think you should make sure
%you get to the point to describe your approach on the first paper}

During the last several years, modern computers are increasingly parallel and
heterogeneous~\cite{mohanty12,misailovic13}.  With parallel architectures being
widely spread, parallel programming must extend beyond its traditional realm of
scientific applications~\cite{li09}.  The ultimate goal of a programmer in a
modern computing environment is to write scalable applications that can take
complete advantage of all available resources, since parallelization is an
important step that can have serious impact on performance and power
consumption~\cite{cockx10}.  However, parallel programming is known to be
difficult and error prone, imposing several challenges to the
programmer~\cite{cockx10,mccool10,mccool12}.  Thus, the difficulty of
developing parallel software is a key obstacle for an average programmer to
exploit the considerable computational power that different multiprocessors
offer~\cite{misailovic13}.

In order to automatically leverage all available resources from parallel
architectures, automatic parallelization is an optimization that produces
parallel code that is semantically equivalent to a sequential code received as
input.  However, automatic parallelization is a difficult problem since the
parallel code must be correct and also perform efficiently on the target
machine~\cite{williams99}.  Despite intense research interest in the area, it
has failed to deliver outside niche domains~\cite{tournavitis09,wang14a}.
Improving the performance of programs for multiprocessors presents a challenge
because even when thread-level parallelism exists, it is difficult for a
compiler to analyze and identify all true dependencies among potential parallel
threads~\cite{hammond98}.

Furthermore, although this area have been widely studied, there is still much
to be explored regarding automatic parallelization that also optimize for
energy consumption, embedded systems~\cite{cordes10} or other emergent
computing architectures~\cite{baskaran10}, such as massively parallel
processors including graphic processing units (GPUs), many integrated cores
(MICs), and others.

Considering that most programs are executed indefinitely many more times than
they are compiled, compilers can afford performing expensive optimization
during compilation time.  Previous work suggest different approaches for
automatic parallelization based on machine learning
techniques~\cite{tournavitis09,wang14a} and also evolutionary
computing~\cite{walsh95,walsh96,williams96,williams99}.

We propose to investigate optimistic approaches, based on machine learning and
evolutionary computing, for automatic parallelization.  The optimistic approach
allows to overcome limitations from traditional approaches based only on static
analysis, taking further advantages from the available heterogeneous
architecture, regarding parallelism and power consumption.

%\FIXME{This section should answer four questions:
%1. Why bother?  (What problems the academic/industry will have if 
%this problem is not solved?
%2. Why it has been solved yet?
%3. How will you solve this problem? (This is missing)
%4. How much better the world will be if the proposed approach works? (Summarize your impact.)
%}
%
%\FIXME{ZW: I think this section should be shorten and focus answering the previous
%four questions. Try to use terms that can be understood by a layman.}

\section{Related work}

In this section we present an overview of several work related to automatic
parallelization of sequential code, where we discuss several parallelization
approaches for both multi-core and heterogeneous systems.

\subsection{Automatic parallelism based on static analysis}

Numerous early work make use of static analysis techniques in order to
parallelize affine
loops~\cite{kuck81,padua93,blume94,lim97,bondhugula08,misailovic13}.  However,
automatic parallelization based on static analysis is challenging and limited
in the type of code that they can
parallelize~\cite{bruening98,kennedy01,chen03}.  In order to identify parallel
regions of code they must use complex interprocedural analyses to prove that
the code has no data dependencies for all possible inputs.  Typical code
targeted by these compilers consists of nested loops with affine array accesses
written in a language with limited aliasing. Large systems written in modern
languages usually contain multiple modules and memory aliasing, which makes
them not amenable to automatic parallelization.  Furthermore, code whose memory
access patterns are indeterminable at compile time due to dependence on program
inputs can be impossible for these compilers to parallelize~\cite{bruening98}.

\subsection{Thread-level speculative parallelism}

In contrast to automatic parallelization that aims at generating a parallel
code, thread-level speculation (TLS) approaches performs sequential code
parallelization during runtime in a speculative manner.  It consists of
selecting regions of code to execute in parallel, usually relying on specilized
hardware to detect dependence violations and re-execute the conflicting
sections such that sequential execution semantics is
preserved~\cite{hammond98,chen03,wu08}.  Although TLS overcomes limitations
intrinsic with conservative compile-time automatic parallelizing tools by
extracting parallel threads optimistically and only ensuring absence of data
dependence violations at runtime, TSL usually requires a specialized hardware
and The overheads associated with maintaining speculative state is a
significant barrier for its usage~\cite{yiapanis13}.

\subsection{Machine learning based approach}

In addition to parallelism detection, automatic parallelization mechanisms must
also determine whether it is profitable to parallelize a loop and how the loop
should be scheduled if a parallel execution is profitable.  Recent
work~\cite{wang09,tournavitis09,wang14a} have been using machine learning
techniques for integrating automatic portable mapping with automatic
parallelism discovery.  Prediction mechanisms are applied to each parallel loop
candidate, deciding if and how the parallel mapping should be performed.

\subsection{Automatic parallelism based on evolutionary computing}

Evolutionary computing has previously been applied to automatic parallelization
and also several other optimizations in
compilers~\cite{walsh95,walsh96,williams96,williams99,schulte14a}.  Previous
work on evolutionary parallelization makes no attempt to analyse the original
code, instead it attempts to directly performs both loops and instructions
parallelization.  Evolutionary based approaches have an optimistic
parallelization strategy, experimentally validating the best individuals.
Although this approach is not limited by dependence analysis techniques, the
evolutionary approach produces a probabilistic result, only with statistical
guarantees.  The correctness of the resulting parallel code is determined by
the fitness function, which compares the output produced by the original
sequential code and the output produced by each parallel code generated.

\subsection{Automatic parallelization for heterogeneous systems}

Although most studies focus on automatic parallelization for homogenous
shared-memory architectures (CPU only), there have been some recent
work~\cite{leung09,baskaran10,amini12,govindarajan13} toward parallelization
for heterogeneous architectures (usually with GPUs).  The usual approach
consists in detecting affine loops which can be parallelized and executed more
efficiently on the GPU rather than on the CPU.  There have been work more
focused on both low-level transformations (e.g. Leung~\etal~\cite{leung09}
extend the Java virtual machine for parallelizing Java bytecodes for GPUs) and
high-level transformations (e.g. Baskaran~\etal~\cite{baskaran08,baskaran10}
propose a source-to-source transformation, using a polyhedral compiler model,
also parallelizing for GPUs).

\subsection{Performance versus accuracy trade-off}
\label{subsec:perfvsacc}

There have been numerous work regarding the trade-off between performance and
accuracy in different areas.  In contrast with traditional approaches, several
work discuss probabilistic and approximate architectures in order to design
hardwares with improved performance regarding power consumption, execution
time, physical space and architectural
simplicity~\cite{palem09,palem12,lingamneni12,kirsch12}.  Some recent work also
propose numerous accuracy-aware code transformations with improved performance,
regarding execution time and resource consumptions, while producing small
accuracy losses~\cite{misailovic11,douskos11,zhu12}.

\section{Research Themes}\label{sec:research-dir}

%\FIXME{ZW: Try to summarize your approach in the first paragraph of this section}

It is widely known that detecting parallelism with static compilers in large,
general programs is challenging~\cite{kennedy01,chen03}.  In order to overcome
the limitations of automatic parallelization based on static analysis for
general multiprocessors, we intend to explore optimistic approaches that aim at
parallelizing aggressively rather than
conservatively~\cite{chen03,williams96,williams99}, such as evolutionary
approaches aided by profiling and machine learning techniques. 

%\FIXME{Breakdown your approach to research themes. Make it 
%clear what problem each theme tries to solve and what's the
%output of each theme.}

\subsection{Theme 1: An optimization model}

In order to overcome the limitations of automatic parallelization based on
static analysis, we intend to define an optimization model for parallelization
based on discrete optimization. A discrete optimization model allows for using
several integer programming techniques, such as evolutionary computing,
simulated annealing, artificial neural networks and others.  While this
approach is not limited by dependence analysis mechanisms, several challenges
need to be addressed.

During the optimization process, each intermediate solution is evaluated by
means of profiling techniques.  Efficiently profiling each individual is
important for reducing the latency involved in the overall parallelization
process using the evolutionary approach.  Some of the strategies for
efficiently profiling include sampling or estimating the
performance~\cite{douskos11,misailovic11,zhu12,fahringer95b,fahringer00,fahringer11}.

\subsection{Theme 2: Parallelization for heterogeneous systems and energy
consumption}

Although automatic parallelization have been widely studied, there is still
much to be explored regarding energy consumption, embedded~\cite{cordes10} or
emergent computing
architectures~\cite{leung09,baskaran10,amini12,govindarajan13}, such as
massively parallel processors including graphic processing units (GPUs) or many
integrated cores (MICs).

Several new challenges are added when parallelizing for heterogeneous systems.
In addition to identifying parallelizable loops, it must also decide, for each
loop, if the loop parallelization will be profitable, and if so, where it
should be executed, i.e., whether to execute it on the CPU, the GPU, or any
other available parallel processor~\cite{tournavitis09,wang14a,leung09}.  In a
heterogeneous system, besides the raw performance of each processor,
synchronization, data communication or even energy consumption must also be
considered when evaluating how the parallel mapping should be performed.

We intend to extend the discrete optimization model in order to support
energy-aware parallelizations, besides integrating with heterogeneous
architectures, such as MICs and GPUs.  The optimization model must consider the
parallelization profitability for the available architecture,
resulting in a model which encompass portable mapping.

\subsection{Theme 3: Solving dependence violations during runtime}

%By modeling the automatic parallelization as an optimization problem,
%defining a parallelization search space in an optimistic manner, both the
%evolutionary and the machine learning based approaches will produce
%probabilistic solutions.

Optimistic approaches for automatic parallelizationg produce probabilistic
solutions.  If a probabilistic solution is unacceptable, we can use speculative
techniques for detecting dependence violations during runtime for re-execution
of conflicting sections providing precise solutions.  This dependence
monitoring mechanism can be implemented entirely in software, similar to
Softspec~\cite{bruening98}, allowing this technique to be used in general
parallel hardware.  Although this mechanism guarantee correctness, it adds some
overhead to the overall execution, which needs to be properly balanced for
performance.

It is also possible to integrate techniques from dynamic parallelization, such
as those used by the \emph{hybrid analysis}~\cite{rus03}, which combines static
and runtime analysis of memory references into a single framework.  In the
context of heterogeneous systems, Govindarajan and
Anantpur~\cite{govindarajan13} describe an algorithm which is able to compute
memory dependencies at runtime, where dependence computation and actual
computation are pipelined on a GPU.

\section{Research plan}

\begin{description}

\item[Year 1] I will perform an extensive literature review in the area,
identifying weaknesses and strengths of current approaches for automatic
parallelization.  The discrete optimization model (Theme 1) is intended to be
developed during the first year.

\item[Year 2] I will extend the optimization model regarding energy
consumption.  Afterwards, the automatic parallelization will be integrated with
heterogeneous architectures (Theme 2).  One or two papers are expected during
the second year.   

\item[Year 3] During the third year, the optimistic automatic parallelization
will be extended with solutions for dependence violations during runtime (Theme
3).  One or two papers as well as the PhD thesis are expected during the final
year.

\end{description}

\section{Expected results}

In this research, we expect to improve the state-of-the-art in energy-aware
automatic parallelization for heterogeneous systems, overcoming some of the
limitations in current dependence analysis mechanisms.  The results produced by
this research should be published in the main journals and conferences of the
area.

\FIXME{We need a conclusion section and timetable here}

\bibliographystyle{abbrv}
\bibliography{ref}
\end{document}
